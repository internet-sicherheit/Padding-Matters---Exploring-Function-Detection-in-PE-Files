#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""dia2dump_parser.py: Extracts information about functions of a DIA2Dump output file, generated by "Dia2Dump.exe <samplename>" and saves the information in a CSV file."""

# Imports
import re
import os
import sys
import time
import pefile
import logging
import pandas as pd
from enum import Enum

# Database table structure
DATABASE_TABLE_STRUCTURE = ['sha256','function_start','tool','tag','rva','name','section','len','function_end','rva_end']
DATABASE_NULL_TYPE =  None

# Tooling
PDB_PARSER_TOOL = "dia2dumpv2"
SHA256SUM_TOOL = "sha256sum"

# States of the state machine
STATE = Enum("State", ["searching",
                       "found_thunk_section",
                       "found_thunk",
                       "found_globals_section",
                       "found_global",
                       "found_public_symbols_section",
                       "found_public_symbol",
                       "found_contribution_section",
                       "found_contribution",
                       "found_memory_map_section",
                       "found_memory_map",
                       "exit"])

# Encoding
ENCODING = "ISO-8859-1"

# Output filename
OUTPUT_FILE_NAME = f".pdb.{PDB_PARSER_TOOL}.parser.csv"

# Strings for function identfication
SPLIT_CHAR = " "
EXPECTED_INDEX = 0
SPLIT_CHAR_SECTION = ":"

# Identifiers for public symbols
PUBLIC_SYMBOLS_SECTION_IDENTIFIER = "*** PUBLICS\n"
PUBLIC_SYMBOLS_IDENTIFIER = "PublicSymbol:"

# Identifiers for thunk section
THUNK_SECTION_IDENTIFIER = "** Module: * Linker *\n"
THUNK_IDENTIFIER = "Thunk"

# Identifiers for globals section
GLOBALS_SECTION_IDENTIFIER = "*** GLOBALS\n"
GLOBALS_IDENTIFIER = "Function:"

#Identifiers for memory map
MEMORY_MAP_SECTION_IDENTIFIER = "** Module: * Linker *\n"
MEMORY_MAP_IDENTIFIER = "CoffGroup"

#Identifiers for section contribution
CONTRIBUTION_SECTION_IDENTIFIER = "*** SECTION CONTRIBUTION\n"
CONTRIBUTION_IDENTIFIER = "  "

#Identifiers for an ending section
END_OF_SECTION = "\n"

# Pattern for regex
GLOBALS_REGEX_PATTERN = r"(\[\b[A-Z0-9]+\])(\[\b[A-Z0-9]+:\b[A-Z0-9]+\])(.+)"
THUNK_REGEX_PATTERN = r"(\[\b[A-Z0-9]+\])(\[\b[A-Z0-9]+:\b[A-Z0-9]+\])(.+)"
THUNK_TARGET_REGEX_PATTERN =  r"(\[\b[A-Z0-9]+\])(\[\b[A-Z0-9]+:\b[A-Z0-9]+\])"
PUBLIC_SYMBOLS_REGEX_PATTERN = r"(\[\b[A-Z0-9]+\])(\[\b[A-Z0-9]+:\b[A-Z0-9]+\])(.+)"
CONTRIBUTION_SECTION_REGEX_PATTERN = r"[^\S]{2}(\b[A-Z0-9]+)[^\S]{2}(\b[A-Z0-9]+:\b[A-Z0-9]+)[^\S]{2}(\b[A-Z0-9]+)[^\S]{2}(.+)"
MEMORY_MAP_TEXT_SECTION_REGEX_PATTERN = r"(.+)(\[\b[a-zA-Z0-9]+:\b[a-zA-Z0-9]+\])(.+)(\.text.*)"
START = 1
END = -1

# Group ids of the regex results
REGEX_FUNCTION_ADDRESS = 1
REGEX_FUNCTION_SECTION = 2
REGEX_FUNCTION_NAME = 3
REGEX_FUNCTION_LENGTH = 3

# Set for text section identifiers
TEXT_SECTION = set()

# Lists for function boundaries
functions_thunks = list()
functions_targets = list()
functions_globals = list()
functions_public_symbols = list()
functions_contribution_sections = list()
functions_modules = list()
functions_modules_thunks = list()
# Logging
formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(funcName)-30s %(message)s')
fh = logging.FileHandler("%s.log" % (os.path.basename(__file__)))
sh = logging.StreamHandler()
fh.setLevel(logging.DEBUG)      # set the log level for the log file
fh.setFormatter(formatter)
sh.setFormatter(formatter)
sh.setLevel(logging.INFO)       # set the log level for the console
logger = logging.getLogger(__name__)
logger.addHandler(fh)
logger.addHandler(sh)
logger.setLevel(logging.DEBUG)
logger.propagate = False

def print_required_time(start_time):
    t_sec = round(time.time() - start_time)
    (t_min, t_sec) = divmod(t_sec,60)
    (t_hour,t_min) = divmod(t_min,60)
    logger.debug(f'Required time: {t_hour}h : {t_min}m : {t_sec}s') 
    print(f'Required time: {t_hour}h : {t_min}m : {t_sec}s')

    
def check_args():
    """ Checking arguments """
    if len(sys.argv)!= 4:
        print(f"Usage: python3 dia2dump_parser.py <PATH/TO/PE> <PATH/TO/DIA2DUMP.txt> [options]")
        print(f"t: thunk functions and targets")
        print(f"g: globals")
        print(f"p: public symbols")
        print(f"c: section contributions")
        print(f"m: module functions")
        exit(-1)
    return (sys.argv[1], sys.argv[2], sys.argv[3])
    
def __update_progress_bar(progress_bar):
    """ Progress bar """
    print(f"\rProgress: [{'#' * int(progress_bar * 20)}{' ' * (20 - int(progress_bar * 20))}] {int(progress_bar*100)}%", end="", flush=True) 

def __extract_identifiers_for_text_section(file_path):
    """ Opens the dia2dump file and searches for the memory map section, that is used to identifiy functions from .text section """
    text_section_names = set()
    file_name = file_path.split("/")[-1]
    logger.debug(f"Opening {file_path} ...")
    with open(file_path, 'r', encoding=ENCODING) as file:
        logger.debug(f"Start searching ...")
        number_of_lines = len(file.readlines())
        file.seek(0)
        
        machine_state = STATE.searching
        
        for i, line in enumerate(file):
            if machine_state == STATE.searching:
                if line == MEMORY_MAP_SECTION_IDENTIFIER:
                    machine_state = STATE.found_memory_map_section
                    logger.debug(f" found MEMORY_MAP_SECTION_IDENTIFIER at line: {i+1}")
                    
            elif machine_state == STATE.found_memory_map_section or machine_state == STATE.found_memory_map:
                if line.split(SPLIT_CHAR)[EXPECTED_INDEX] == MEMORY_MAP_IDENTIFIER:
                    machine_state = STATE.found_memory_map
                    regex_result = re.search(MEMORY_MAP_TEXT_SECTION_REGEX_PATTERN, line)
                    if regex_result is not None:
                        TEXT_SECTION.add(regex_result.group(REGEX_FUNCTION_SECTION)[START:END].split(SPLIT_CHAR_SECTION)[0])
                        text_section_names.add(regex_result.group(4))
                elif machine_state == STATE.found_memory_map and line == END_OF_SECTION:
                    machine_state = STATE.exit
            
            elif machine_state == STATE.exit:
                break
            __update_progress_bar((i+1)/(number_of_lines*2))
    __update_progress_bar(0.5)            
    file.close()
    
    tmp_text_section = set()
    for section_identifier in TEXT_SECTION:
        if section_identifier[0:2] == "0x":
            tmp_text_section.add(section_identifier[2:])
        else:
            tmp_text_section.add(f"0x{section_identifier}")

    TEXT_SECTION.update(tmp_text_section)    
    logger.debug(f" found following text sections: {text_section_names} with identifiers: {TEXT_SECTION}")	
	            
def __get_next_state(i, line, state):
    """ Checks whether the state should be maintained. Needed for e.g. an empty section """
    machine_state = None
    if line == GLOBALS_SECTION_IDENTIFIER:
        logger.debug(f" found GLOBALS_SECTION_IDENTIFIER at line: {i+1}")
        machine_state = STATE.found_globals_section
    elif line == THUNK_SECTION_IDENTIFIER:
        logger.debug(f" found THUNK_SECTION_IDENTIFIER at line: {i+1}")
        machine_state = STATE.found_thunk_section
    elif line == PUBLIC_SYMBOLS_SECTION_IDENTIFIER:
        logger.debug(f" found PUBLIC_SYMBOLS_SECTION_IDENTIFIER at line: {i+1}")
        machine_state = STATE.found_public_symbols_section
    elif line == CONTRIBUTION_SECTION_IDENTIFIER:
        logger.debug(f" found CONTRIBUTION_SECTION_IDENTIFIER at line: {i+1}")
        machine_state = STATE.found_contribution_section
    else:
        machine_state = state
    return machine_state
    
def __state_globals(line, machine_state):
    """ Executed if globals were found """
    new_machine_state = machine_state
    if line.split(SPLIT_CHAR)[EXPECTED_INDEX] == GLOBALS_IDENTIFIER:
        new_machine_state = STATE.found_global
        regex_result = re.search(GLOBALS_REGEX_PATTERN, line)
        function_address_global = regex_result.group(REGEX_FUNCTION_ADDRESS)[START:END]
        function_address_global_section = regex_result.group(REGEX_FUNCTION_SECTION)[START:END].split(SPLIT_CHAR_SECTION)[0]
        function_address_global_name = regex_result.group(REGEX_FUNCTION_NAME).strip()
        if function_address_global_section in TEXT_SECTION:
            functions_globals.append((function_address_global,int(function_address_global_section,16), function_address_global_name, "globals",DATABASE_NULL_TYPE))
    elif machine_state == STATE.found_global and line == END_OF_SECTION:
        new_machine_state = STATE.searching
        
    return new_machine_state

def __state_section_contribution(line, machine_state):
    """ Executed if section contruibiton was found """
    new_machine_state = machine_state
    if line[0:2] == CONTRIBUTION_IDENTIFIER and line[0:4] != CONTRIBUTION_IDENTIFIER*2:
        new_machine_state = STATE.found_contribution
        regex_result = re.search(CONTRIBUTION_SECTION_REGEX_PATTERN, line)
        function_address_contribution = regex_result.group(REGEX_FUNCTION_ADDRESS).strip()
        function_contribution_section = regex_result.group(REGEX_FUNCTION_SECTION).split(SPLIT_CHAR_SECTION)[0].strip()
        function_length = int(regex_result.group(REGEX_FUNCTION_LENGTH).strip(),16)

        if function_contribution_section in TEXT_SECTION and function_length > 0:
            functions_contribution_sections.append((function_address_contribution, int(function_contribution_section,16), DATABASE_NULL_TYPE, "section_contribution",DATABASE_NULL_TYPE))
    elif machine_state == STATE.found_contribution and line == END_OF_SECTION:
        new_machine_state = STATE.searching
    
    return new_machine_state

def __state_publics(line, machine_state):
    """ Executed if publics were found """
    new_machine_state = machine_state
    if line.split(SPLIT_CHAR)[EXPECTED_INDEX] == PUBLIC_SYMBOLS_IDENTIFIER:
        new_machine_state = STATE.found_public_symbol
        regex_result = re.search(PUBLIC_SYMBOLS_REGEX_PATTERN, line)
        function_public_symbol = regex_result.group(REGEX_FUNCTION_ADDRESS)[START:END]
        function_public_symbol_section = regex_result.group(REGEX_FUNCTION_SECTION)[START:END].split(SPLIT_CHAR_SECTION)[0]
        function_public_symbol_name = regex_result.group(REGEX_FUNCTION_NAME).strip()
        if function_public_symbol_section in TEXT_SECTION:
            functions_public_symbols.append((function_public_symbol, int(function_public_symbol_section,16), function_public_symbol_name, "publics",DATABASE_NULL_TYPE))
    elif machine_state == STATE.found_public_symbol and line == END_OF_SECTION:
        new_machine_state = STATE.searching
        
    return new_machine_state
    
def __state_thunks(line, machine_state):
    """ Exectued if thunk section was found """
    new_machine_state = machine_state
    if line.split(SPLIT_CHAR)[EXPECTED_INDEX] == THUNK_IDENTIFIER:
        new_machine_state = STATE.found_thunk
        regex_result = re.search(THUNK_REGEX_PATTERN, line)
                    
        function_address_thunk = regex_result.group(REGEX_FUNCTION_ADDRESS)[START:END]
        target = (regex_result.group(REGEX_FUNCTION_NAME)).strip()

        text_section_thunk = regex_result.group(REGEX_FUNCTION_SECTION).split(SPLIT_CHAR_SECTION)[0].strip()[1:]
        if text_section_thunk in TEXT_SECTION:
            functions_thunks.append((function_address_thunk, int(text_section_thunk,16), DATABASE_NULL_TYPE, "thunk", DATABASE_NULL_TYPE))
                    
        regex_result = re.search(THUNK_TARGET_REGEX_PATTERN, target)
        function_address_target = regex_result.group(REGEX_FUNCTION_ADDRESS)[START:END]
        text_section_thunk_target = regex_result.group(REGEX_FUNCTION_SECTION).split(SPLIT_CHAR_SECTION)[0].strip()[1:]           
        if text_section_thunk_target in TEXT_SECTION:
            functions_targets.append((function_address_target, int(text_section_thunk_target,16), DATABASE_NULL_TYPE, "thunk_target", DATABASE_NULL_TYPE))
            
    elif machine_state == STATE.found_thunk and line == END_OF_SECTION:
        new_machine_state = STATE.searching
        
    return new_machine_state

def __state_independent(line):
            """ State indepentend operations. Good for hotfixes or testing. """
            
            #ToDo: Use regex
            if line != None and "Function       : " in line:
                if line[len("Function       : ")] != " ":
                    function_entry = line.split(",")
                    addresses = function_entry[1].split("][")
                    module_function_address = (addresses[0])[2:]
                    text_section_module_function = (addresses[1])[:-1].split(":")[0]
                    function_len = function_entry[2][7:]
                    module_function_name = (line[line.find((","),(line.find((","),(line.find(","))+1))+1):])[2:-1]

                    if text_section_module_function in TEXT_SECTION:
                        functions_modules.append((module_function_address, int(text_section_module_function,16), module_function_name, "module_function", function_len))
            
            #ToDo: Use regex
            elif line != None and "Thunk          : [" in line:
                thunk_entry = line.split(",")[0]

                addresses = thunk_entry.split(":")[1].split("][")
                function_address_thunk = (addresses[0])[2:]
                text_section_thunk = (addresses[1]).split(":")[0]
                thunk_targets = line.split(" target ")[1].strip()

                if text_section_thunk in TEXT_SECTION and (thunk_targets[0] != '[' or thunk_targets[0:7] == '[thunk]'):
                    functions_modules_thunks.append((function_address_thunk, int(text_section_thunk,16), thunk_targets, "module_thunk", DATABASE_NULL_TYPE))

                    
def __extract_functions(file_path):
    """ State machine for function extraction. Processes line by line """
    with open(file_path, 'r', encoding=ENCODING) as file:
        number_of_lines = len(file.readlines())
        file.seek(0)
        
        machine_state = STATE.searching
        for i, line in enumerate(file):

            __state_independent(line)
                
            machine_state = __get_next_state(i, line, machine_state)                  
            
            if machine_state == STATE.found_globals_section or machine_state == STATE.found_global:
                machine_state = __state_globals(line, machine_state)
                    
            elif machine_state == STATE.found_contribution_section or machine_state == STATE.found_contribution:
                machine_state = __state_section_contribution(line, machine_state)
                    
            elif machine_state == STATE.found_thunk_section or machine_state == STATE.found_thunk:
                machine_state = __state_thunks(line, machine_state)

            elif machine_state == STATE.found_public_symbols_section or machine_state == STATE.found_public_symbol:
                machine_state = __state_publics(line, machine_state)
                    
            __update_progress_bar((number_of_lines+(i+1))/(number_of_lines*2))
            
    file.close()
    __update_progress_bar(1)
    print()
    logger.debug(f" done with function extraction!")

def filter_results(filter_options):
    """ Filters function addresses """
    logger.debug(f"Extracted function addresses from:")
    function_set = set()
    function_address_set = set()
    
    if 't' in filter_options:
        function_set.update(functions_thunks)
        function_set.update(functions_targets)
        logger.debug(f"- Thunks: {len(set(functions_thunks))}")
        logger.debug(f"- Thunks targets: {len(set(functions_targets))}")
    if 'g' in filter_options: 
        function_set.update(functions_globals)
        logger.debug(f"- Globals: {len(set(functions_globals))}")
    if 'p' in filter_options:
        function_set.update(functions_public_symbols)
        logger.debug(f"- Publics: {len(set(functions_public_symbols))}")
    if 'c' in filter_options:
        function_set.update(functions_contribution_sections)
        logger.debug(f"- Section contribution: {len(set(functions_contribution_sections))}")
    if 'm' in filter_options:
        function_set.update(functions_modules)
        function_set.update(functions_modules_thunks)
        logger.debug(f"- Module function: {len(set(functions_modules))}")
        logger.debug(f"- Module thunk: {len(set(functions_modules_thunks))}")
        

    function_list = list(function_set)
    for item in function_list:
        function_address_set.add(item[0])

    logger.debug(f"Unique function addresses: {len(function_address_set)}")
    print(f"Unique function addresses: {len(function_address_set)}")
    return function_list


def calc_offset(pe_file_path, function_address_list):
    """ Calculates file offsets with pefile """
    logger.debug(f"Calculating file offset...")
    functions = list()
    pe_file = pefile.PE(pe_file_path)
    
    for function_info_tuple in function_address_list:
        f_rva_hex_str, f_section, f_name, f_tag, f_len_hex_str = function_info_tuple
        
        f_rva_int_value = int(f_rva_hex_str,16)
        f_offset = pe_file.get_offset_from_rva(f_rva_int_value)
        
        if f_len_hex_str != DATABASE_NULL_TYPE:
            f_len_int_value = int(f_len_hex_str,16)
            f_rva_len = int(f_rva_int_value) + int(f_len_int_value)
            f_offset_len = int(pe_file.get_offset_from_rva(f_rva_len))
        else:
            f_len_int_value = DATABASE_NULL_TYPE
            f_rva_len = DATABASE_NULL_TYPE
            f_offset_len = DATABASE_NULL_TYPE
            
        functions.append((f_offset, f_rva_int_value, f_tag ,f_name, f_section, f_len_int_value, f_rva_len, f_offset_len))
    return functions

def tag_and_save_function_boundaries(pe_file_path, function_address_list_file_offset):
    """ Tags function boundaries and saves information to disk """    
    file_name = os.path.basename(pe_file_path)
    file_path = os.path.dirname(pe_file_path)

    sha256sum_of_executable = os.popen((f"{SHA256SUM_TOOL} \"{str(pe_file_path)}\"")).read().split(" ")[0]
        
    df = pd.DataFrame(function_address_list_file_offset, columns =['function_start', 'rva', 'tag', 'name', 'section','len','rva_end','function_end'])
    df['sha256'] = [sha256sum_of_executable] * len(function_address_list_file_offset)
    df['tool'] = [PDB_PARSER_TOOL] * len(function_address_list_file_offset)
    df = df.loc[:, DATABASE_TABLE_STRUCTURE]
    
    df['sha256'] = df['sha256'].astype(str)
    df['tool'] = df['tool'].astype(str)
    df['function_start'] = df['function_start'].astype('Int64')
    df['rva'] = df['rva'].astype('Int64')
    df['tag'] = df['tag'].astype(str)
    df['name'] = df['name'].astype(str)
    df['section'] = df['section'].astype('Int64')
    df['len'] = df['len'].astype('Int64')
    df['rva_end'] = df['rva_end'].astype('Int64')
    df['function_end'] = df['function_end'].astype('Int64')
                          
    csv_data = df.to_csv(f"{file_path}/{file_name}{OUTPUT_FILE_NAME}", encoding=ENCODING, index=False, header=False)
    
    logger.debug(f"Saved {len(function_address_list_file_offset)} function boundaries under {file_path}/{file_name}{OUTPUT_FILE_NAME}")
    print(f"Saved {len(function_address_list_file_offset)} function boundaries under {file_path}/{file_name}{OUTPUT_FILE_NAME}")

def extract_function_boundaries(dia2dump_txt, filter_options):
    """ Extractes function boundaries and filters them """
    __extract_identifiers_for_text_section(dia2dump_txt)
    __extract_functions(dia2dump_txt)
    return filter_results(filter_options)
    
def main():
    """ Main program """
    start_time = time.time()
    pe_file_path, dia2dump_txt, filter_options = check_args()
    function_address_list = extract_function_boundaries(dia2dump_txt, filter_options)
    function_address_list_file_offset = calc_offset(pe_file_path,function_address_list)
    tag_and_save_function_boundaries(pe_file_path, function_address_list_file_offset)
    print_required_time(start_time)
    return 0

if __name__ == "__main__":
    main()
    
